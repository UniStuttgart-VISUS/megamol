<?xml version="1.0" encoding="utf-8"?>
<btf type="MegaMolHLSLShader" version="1.0" namespace="d3d11sphere">

  <snippet name="inputlayout-pos4fcol4f">
    <![CDATA[
struct VsInput {
    float4 Position : POSITION;
    float4 Colour : COLOR;
};
    ]]>    
  </snippet>
  
  <snippet name="types" type="string">
    <![CDATA[
struct GsInput {
    float4 Position : POSITION;
    float4 Colour : COLOR;
    float Radius : FOG;
};

struct PsInput {
    float4 Position : SV_POSITION;
    float4 SphereParams : TEXCOORD0;
    float4 Colour : COLOR0;
    nointerpolation float4 CameraPosition : TEXCOORD1;
    nointerpolation float4 CameraDirection : TEXCOORD2;
    nointerpolation float4 CameraUp : TEXCOORD3;
    nointerpolation float4 CameraRight : TEXCOORD4;
    nointerpolation float EyeSeparation : TESSFACTOR0;
    nointerpolation float Convergence : TESSFACTOR1;
};

struct PsOutput {
    float4 Colour : SV_TARGET;
    float Depth : SV_DEPTH;
};
    ]]>
  </snippet>

  <snippet name="sphere-cbuffers" type="string">
    <![CDATA[
cbuffer Constants : register(c0) {
    matrix ProjMatrix;
    matrix ViewMatrix;
    matrix ViewInvMatrix;
    matrix ViewProjMatrix;
    matrix ViewProjInvMatrix;
    float4 Viewport;
    
    // TODO: Remove the following
    float4 CamPos;
    float4 CamDir;
    float4 CamUp;
};
    ]]>
  </snippet>

  <snippet name="bbox-cbuffers" type="string">
    <![CDATA[
cbuffer Constants : register(c0) {
    matrix ViewMatrix;
    matrix ProjMatrix;
    float4 Colour;
};
    ]]>    
  </snippet>

  <snippet name="camera" type="string">
    <![CDATA[
void ReconstructCamera(out float4 pos, out float4 dir, out float4 up, 
        out float4 right, const in matrix viewInvMatrix) {    
    float4 tmp;
        
    // calculate cam position
    pos = viewInvMatrix._41_42_43_44; // (C) by Christoph
    
    // camera coordinate system in object space
    tmp = viewInvMatrix._41_42_43_44 + viewInvMatrix._31_32_33_34;
    dir = normalize(tmp);
    
    tmp = viewInvMatrix._41_42_43_44 + viewInvMatrix._21_22_23_24;
    up = normalize(tmp);
    
    right = float4(normalize(cross(dir.xyz, up.xyz)), 0.0);
    
    up = float4(normalize(cross(dir.xyz, right.xyz)), 0.0);
}
    ]]>
  </snippet>

  <snippet name="lighting" type="string">
    <![CDATA[
// ray:      the eye to fragment ray vector
// normal:   the normal of this fragment
// lightPos: the position of the light source
// color:    the base material color
float3 LocalLighting(const in float3 ray, const in float3 normal, const in float3 lightPos, const in float3 color) {
    // TODO: rewrite!
    float3 lightDir = normalize(lightPos);

    float4 lightparams = float4(0.2, 0.8, 0.4, 10.0);
#define LIGHT_AMBIENT lightparams.x
#define LIGHT_DIFFUSE lightparams.y
#define LIGHT_SPECULAR lightparams.z
#define LIGHT_EXPONENT lightparams.w
    float nDOTl = dot(normal, lightDir);

    float3 r = normalize(2.0 * nDOTl * normal - lightDir);
    return LIGHT_AMBIENT * color 
        + LIGHT_DIFFUSE * color * max(nDOTl, 0.0) 
        + LIGHT_SPECULAR * float3(pow(max(dot(r, -ray), 0.0), LIGHT_EXPONENT).xxx);
}    
    ]]>
  </snippet>

  <snippet name="nvstereo" type="string">
    <![CDATA[
Texture2D TexStereoParams : register(t8);
SamplerState SamplerStereoParams : register(s8);

float4 StereoToMonoClipSpace(float4 stereoClipPos) {
    float4 monoClipPos = stereoClipPos;
    // Take a sample directly in the middle of the pixel at 0, 0, which is 0 / Width + 1 / (Width * 2)
    float2 stereoParms = TexStereoParams.Sample(SamplerStereoParams, float2(0.0625f, 0.5f)).xy;
    float eyeSep = stereoParms.x;
    float convergence = stereoParms.y;
    
    // Stereo transform is: stereoClipPos.x = monoClipPos.x + eyeSep * (monoClipPos.w - convergence);
    monoClipPos.x -= eyeSep * (monoClipPos.w - convergence);
    
    return monoClipPos;
}

float4 ComputeWorldPosition(float4 screenPosition, bool undoMonoToStereoXform) {	
    // First, undo the viewport transform.
    float4 homogenousDeviceSpacePos = float4(2 * (screenPosition.xy - 0.5) / Viewport.zw - 1.0, screenPosition.zw);
    // Viewport space and homogenous device space are inverted with respect to y, so fix that here.
    homogenousDeviceSpacePos.y = -homogenousDeviceSpacePos.y;

    // Then, undo the w-divide. 
    float4 clipspacePos = float4(homogenousDeviceSpacePos.xyz * homogenousDeviceSpacePos.w, homogenousDeviceSpacePos.w);

    if (undoMonoToStereoXform) {
        clipspacePos = StereoToMonoClipSpace(clipspacePos);
    }
    
    // Finally, undo the world space projection
    return mul(clipspacePos, ViewProjInvMatrix);
}
    ]]>    
  </snippet>

  <shader name="debug-vertex">
    <snippet name="sphere-cbuffers" />
    <snippet name="inputlayout-pos4fcol4f" />
    <snippet name="types" />
    <snippet name="main" type="string">
      <![CDATA[
PsInput Main(VsInput input) {
    PsInput retval = (PsInput) 0;
    retval.Position = float4(input.Position.xyz, 1.0);
    retval.Position = mul(retval.Position, ViewMatrix);
    retval.Position = mul(retval.Position, ProjMatrix);
    retval.SphereParams = input.Position;
    return retval;    
}
      ]]>
    </snippet>    
  </shader>

  <shader name="bbox-vertex">
    <snippet name="bbox-cbuffers" />
    <snippet name="inputlayout-pos4fcol4f" />
    <snippet name="main" type="string">
      <![CDATA[
float4 Main(float4 position : POSITION) : SV_POSITION {
    // mul(M, v) -> treat as column vectors
    // mul(v, M) -> treat as row vectors
    float4 retval = position;
    retval = mul(retval, ViewMatrix);
    retval = mul(retval, ProjMatrix);
    return retval;
}
      ]]>
    </snippet>
  </shader>

  <shader name="bbox-pixel">
    <snippet name="bbox-cbuffers" />
    <snippet name="main" type="string">
      <![CDATA[
float4 Main(float4 position : SV_POSITION) : SV_TARGET {
    return Colour;
}
      ]]>
    </snippet>
  </shader>

  <shader name="sphere-vertex">
    <snippet name="sphere-cbuffers" />
    <snippet name="inputlayout-pos4fcol4f" />
    <snippet name="types" />
    <snippet name="main" type="string">
      <![CDATA[
GsInput Main(VsInput input) {
    GsInput retval = (GsInput) 0;
    retval.Position = float4(input.Position.xyz, 1.0f);
    retval.Radius = input.Position.w;
    retval.Colour = input.Colour;
    return retval;
}
      ]]>
    </snippet>
  </shader>

  <shader name="sphere-geometry">
    <snippet name="sphere-cbuffers" />
    <snippet name="types" />
    <snippet name="camera" />
    <snippet name="nvstereo" />
    <snippet name="main" type="string">
      <![CDATA[
[maxvertexcount(4)]
void Main(point GsInput input[1], inout TriangleStream<PsInput> triStream) {
    PsInput v = (PsInput) 0;
    
    // Take a sample directly in the middle of the pixel at 0, 0, which is 0 / Width + 1 / (Width * 2)
    // Note: We changed NVIDIA's linear sampler to a point sampler, so we can
    // just take the pixel at (0, 0).
    float3 stereoParms = TexStereoParams.SampleLevel(SamplerStereoParams, 0.0.xx, 0); //float2(0.0625, 0.5));
    v.EyeSeparation = stereoParms.x;
    v.Convergence = stereoParms.y;

    matrix mvp = ViewProjMatrix;
    float rad = input[0].Radius;
//#define MAJOR_DOWELING_RADIUS    
#ifdef MAJOR_DOWELING_RADIUS
    rad *= 5;
#endif
    float squareRad = rad * rad;

    float4 objPos = input[0].Position;
    objPos.w = 1.0;
    v.Colour = input[0].Colour;
    v.SphereParams = float4(input[0].Position.xyz, rad);
    
    // Reconstruct camera system.
    ReconstructCamera(v.CameraPosition, v.CameraDirection, v.CameraUp, 
      v.CameraRight, ViewInvMatrix);

    // Transform camera to glyph space and undo stereo transform.
    v.CameraPosition.xyz -= objPos.xyz;
    v.CameraPosition.xyz += v.CameraRight * v.EyeSeparation;

    // SphereParams-Touch-Plane-Approachâ„¢
    float2 winHalf = 2.0 / Viewport.zw; // window size

    float2 d, p, q, h, dd;

    float2 mins, maxs;
    float3 testPos;
    float4 projPos;

#ifdef HALO
    squarRad = (rad + HALO_RAD) * (rad + HALO_RAD);
#endif // HALO

#if 0
    //bottom left
    v.Position = mul(objPos - v.CameraUp * 1.5 * rad - v.CameraRight * 1.5 * rad, mvp);
    triStream.Append(v);
 
    //top left
    v.Position = mul(objPos + v.CameraUp * 1.5 * rad - v.CameraRight * 1.5 * rad, mvp);
    triStream.Append(v);
 
    //bottom right
    v.Position = mul(objPos - v.CameraUp * 1.5 * rad + v.CameraRight * 1.5 * rad, mvp);
    triStream.Append(v);
 
    //top right
    v.Position = mul(objPos + v.CameraUp * 1.5 * rad + v.CameraRight * 1.5 * rad, mvp);
    triStream.Append(v);
    
    triStream.RestartStrip();
    
#else

    // projected camera vector
    float3 c2 = float3(dot(v.CameraPosition.xyz, v.CameraRight),
      dot(v.CameraPosition.xyz, v.CameraUp),
      dot(v.CameraPosition.xyz, v.CameraDirection));
    
    float3 cpj1 = v.CameraDirection * c2.z + v.CameraRight * c2.x;
    float3 cpm1 = v.CameraDirection * c2.x - v.CameraRight * c2.z;
    
    float3 cpj2 = v.CameraDirection * c2.z + v.CameraUp * c2.y;
    float3 cpm2 = v.CameraDirection * c2.y - v.CameraUp * c2.z;
    
    d.x = length(cpj1);
    d.y = length(cpj2);

    dd = 1.0.xx / d;

    p = squareRad * dd;
    q = d - p;
    h = sqrt(p * q);
    //h = float2(0.0);
    
    p *= dd;
    h *= dd;

    cpj1 *= p.x;
    cpm1 *= h.x;
    cpj2 *= p.y;
    cpm2 *= h.y;

    testPos = objPos.xyz + cpj1 + cpm1;
    projPos = mul(float4(testPos, 1.0), mvp);
    ///projPos = mul(mvp, float4(testPos, 1.0));
    projPos /= projPos.w;
    mins = projPos.xy;
    maxs = projPos.xy;

    testPos -= 2.0 * cpm1;
    projPos = mul(float4(testPos, 1.0), mvp);
    ///projPos = mul(mvp, float4(testPos, 1.0));
    projPos /= projPos.w;
    mins = min(mins, projPos.xy);
    maxs = max(maxs, projPos.xy);

    testPos = objPos.xyz + cpj2 + cpm2;
    projPos = mul(float4(testPos, 1.0), mvp);
    ///projPos = mul(mvp, float4(testPos, 1.0));
    projPos /= projPos.w;
    mins = min(mins, projPos.xy);
    maxs = max(maxs, projPos.xy);

    testPos -= 2.0 * cpm2;
    projPos = mul(float4(testPos, 1.0), mvp);
    ///projPos = mul(mvp, float4(testPos, 1.0));
    projPos /= projPos.w;
    mins = min(mins, projPos.xy);
    maxs = max(maxs, projPos.xy);
    
    //bottom left
    v.Position = float4(mins.x, mins.y, projPos.z, 1.0);
    triStream.Append(v);
 
    //top left
    v.Position = float4(mins.x, maxs.y, projPos.z, 1.0);
    triStream.Append(v);
 
    //bottom right
    v.Position = float4(maxs.x, mins.y, projPos.z, 1.0);
    triStream.Append(v);
 
    //top right
    v.Position = float4(maxs.x, maxs.y, projPos.z, 1.0);
    triStream.Append(v);
    
    triStream.RestartStrip();
#endif
}
      ]]>
    </snippet>
  </shader>

  <shader name="debug-pixel">
    <snippet name="sphere-cbuffers" />
    <snippet name="types" />
    <snippet name="main" type="string">
      <![CDATA[
float4 Main(PsInput input) : SV_Target {
      return float2(1.0, 0.0).xyyx;
}
      ]]>
    </snippet>
  </shader>

  <shader name="sphere-pixel">
    <snippet name="sphere-cbuffers" />
    <snippet name="types" />
    <snippet name="nvstereo" />
    <snippet name="lighting" />
    
    <snippet name="main" type="string">
      <![CDATA[  
PsOutput Main(PsInput input) {
    PsOutput retval = (PsOutput) 0;
    
    float4 coord;
    float3 ray;
    float lambda;
    float3 sphereintersection = 0.0.xxx;
    float3 normal;
  
    float4 camPos = input.CameraPosition;
    float4 camIn = input.CameraDirection;
    float4 camUp = input.CameraUp;
    float4 camRight = input.CameraRight;
    float eyeSep = input.EyeSeparation;

    float4 objPos = float4(input.SphereParams.xyz, 1.0);

// TODO
    float4 lightPos = normalize(float4(0.5, -1.0, -1.0, 0));
    lightPos *= -1;
    lightPos = mul(lightPos, ViewInvMatrix);
    float rad = input.SphereParams.w;
    float squarRad = rad * rad;

    // transform fragment coordinates from window coordinates to view coordinates.
    // TODO: Replace NV-stuff with own implementation to save texture sampling
#define STEREO_WORLD_POS
#ifdef STEREO_WORLD_POS
    coord = ComputeWorldPosition(input.Position, true);
#else    
    input.Position.y = Viewport.w - input.Position.y;
    coord = input.Position
        * float4(2.0 / Viewport.z, 2.0 / Viewport.w, 1.0, 0.0) 
        + float4(-1.0, -1.0, 0.0, 1.0);

    // transform fragment coordinates from view coordinates to object coordinates.
    coord = mul(coord, ViewProjInvMatrix);
 #endif
    coord /= coord.w; 
    coord -= objPos; // ... and to glyph space
    
    // calc the viewing ray
    ray = coord.xyz - camPos.xyz;
    ray = normalize(ray);

    // calculate the geometry-ray-intersection
    float d1 = -dot(camPos.xyz, ray);                       // projected length of the cam-SphereParams-vector onto the ray
    float d2s = dot(camPos.xyz, camPos.xyz) - d1 * d1;      // off axis of cam-SphereParams-vector and ray
    float radicand = squarRad - d2s;                        // square of difference of projected length and lambda
    lambda = d1 - sqrt(radicand);                           // lambda

    if (radicand < 0.0) {
        discard;
        //retval.Colour = 0.8.xxxx;
        //retval.Depth = input.Position.z;
        //return retval;
    } else {
        // chose color for lighting
        sphereintersection = lambda * ray + camPos.xyz;    // intersection point
        // "calc" normal at intersection point
        normal = sphereintersection / rad;
        //normal = normalize(sphereintersection);
        // phong lighting with directional light
        retval.Colour = float4(LocalLighting(ray, normal, lightPos.xyz, input.Colour.rgb), input.Colour.a);  
    }
   
// calculate depth
#define DEPTH
#ifdef DEPTH
    float4 Ding = float4(sphereintersection + objPos.xyz, 1.0);
    float depth = dot(ViewProjMatrix._13_23_33_43, Ding);
    float depthW = dot(ViewProjMatrix._14_24_34_44, Ding);
    //retval.Depth = ((depth / depthW) + 1.0) * 0.5;
    retval.Depth = (depth / depthW);
#else
    retval.Depth = input.Position.z;
#endif // DEPTH    

    return retval;
}
      ]]>
    </snippet>
  </shader>
</btf>
